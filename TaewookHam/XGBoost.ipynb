{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf413c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing import List, Any, Tuple\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5363f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "X_model = pd.read_csv('X_model.csv')\n",
    "Y_model = pd.read_csv('Y_model.csv')\n",
    "scaler = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4096fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining preprocessors...\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessors\n",
    "print(\"Defining preprocessors...\")\n",
    "def column(colnames: List[str]):\n",
    "    def _column(X: pd.DataFrame):\n",
    "        X = X.fillna(0)\n",
    "        return [\n",
    "            [colname, X[colname].values] for colname in colnames\n",
    "        ]\n",
    "    return _column\n",
    "\n",
    "def rangesum(\n",
    "    name:str, \n",
    "    regex: str, \n",
    "    prefixes: str, \n",
    "    dist: np.ndarray\n",
    "):\n",
    "    def _rangesum(X: pd.DataFrame):\n",
    "        X = X.fillna(0)\n",
    "        return [\n",
    "            [\n",
    "                prefix + name, \n",
    "                X.filter(regex=(prefix + regex), axis=1).values.dot(dist)\n",
    "            ] for prefix in prefixes\n",
    "        ]\n",
    "    return _rangesum\n",
    "\n",
    "def _fillna(X: np.ndarray) -> np.ndarray:\n",
    "    return np.nan_to_num(X, copy=True, nan=0)\n",
    "\n",
    "def array_divide(\n",
    "    numerator: List[Tuple[str, np.ndarray]], \n",
    "    denominator: List[Tuple[str, np.ndarray]]\n",
    ") -> List[Any]:\n",
    "    assert len(numerator) == len(denominator)\n",
    "    return [\n",
    "        [\n",
    "            \"r\" + numerator_colname, \n",
    "            _fillna(np.divide(numerator_col, denominator_col))\n",
    "        ] for [numerator_colname, numerator_col], [_, denominator_col] in zip(numerator, denominator)\n",
    "    ]\n",
    "\n",
    "def one_hot_encode(column: str) -> pd.DataFrame:\n",
    "    def _one_hot_encode(X: pd.DataFrame):\n",
    "        X = X.fillna(0)\n",
    "        df_dummies = pd.get_dummies(X[column], prefix=column)\n",
    "        return [\n",
    "            [colname, df_dummies[colname].values] for colname in df_dummies.columns\n",
    "        ]\n",
    "    return _one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bece555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X: pd.DataFrame, processors: List[Any]) -> pd.DataFrame:\n",
    "    X_new = pd.DataFrame()\n",
    "\n",
    "    for processor in processors:\n",
    "        for colname, col in processor if type(processor) == type([]) else processor(X):\n",
    "            X_new[colname] = col\n",
    "\n",
    "    X_new = X_new.fillna(0)\n",
    "\n",
    "#     X_new = pd.DataFrame(scaler.fit_transform(X_new), columns=X_new.columns)\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bbd47e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_dist(length: int) -> np.ndarray:\n",
    "    return np.ones(length)\n",
    "\n",
    "def linear_dist(length: int) -> np.ndarray:\n",
    "    return np.arange(start=0, stop=1, step=1/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c72f04a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing...\n"
     ]
    }
   ],
   "source": [
    "print(\"Data preprocessing...\")\n",
    "dist_GIT = rangesum(\n",
    "    'GIT', \n",
    "    r\"202205[0-9]{2}\", \n",
    "    \"cts\", \n",
    "    equal_dist(31)\n",
    ")(X_model)\n",
    "dist_VAT = rangesum(\n",
    "    'VAT', \n",
    "    r\"20220[17](?:[01][0-9]|2[0-5])\", \n",
    "    \"ts\", \n",
    "    np.concatenate((equal_dist(25), equal_dist(25)))\n",
    ")(X_model)\n",
    "entire_days = 31 + 29 + 31 + 30 + 31 + 30 + 31 + 25\n",
    "entire = rangesum(\n",
    "    'Entire', \n",
    "    r\"2022[0-9]{4}\", \n",
    "    \"cts\", \n",
    "    equal_dist(entire_days)\n",
    ")(X_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4af3390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['cGIT', array([ 0.,  2.,  6., ..., 25.,  0.,  6.])], ['tGIT', array([0., 0., 3., ..., 8., 0., 1.])], ['sGIT', array([   0.,   17., 2253., ..., 3395.,    0.,  448.])]]\n",
      "[['gender', array([13,  5,  6, ...,  4,  7,  7], dtype=int64)], ['age_code', array([13,  5,  6, ...,  4,  7,  7], dtype=int64)], ['region_code', array([7, 1, 2, ..., 7, 1, 1], dtype=int64)]]\n"
     ]
    }
   ],
   "source": [
    "age_code = np.array(X_model['age_code'])\n",
    "gender_code = np.array(X_model['gender'])\n",
    "region_code = np.array(X_model['region_code'])\n",
    "cat_Featrues = []\n",
    "cat_Featrues.append(['gender',age_code])\n",
    "cat_Featrues.append(['age_code',age_code])\n",
    "cat_Featrues.append(['region_code',region_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "89ac2b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age_code</th>\n",
       "      <th>region_code</th>\n",
       "      <th>cGIT</th>\n",
       "      <th>tGIT</th>\n",
       "      <th>sGIT</th>\n",
       "      <th>tVAT</th>\n",
       "      <th>sVAT</th>\n",
       "      <th>cEntire</th>\n",
       "      <th>tEntire</th>\n",
       "      <th>sEntire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1712.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5119.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  age_code  region_code  cGIT  tGIT    sGIT  tVAT    sVAT  cEntire  \\\n",
       "0      13        13            7   0.0   0.0     0.0   0.0     0.0      1.0   \n",
       "1       5         5            1   2.0   0.0    17.0   0.0   185.0     39.0   \n",
       "2       6         6            2   6.0   3.0  2253.0   0.0  1712.0     26.0   \n",
       "\n",
       "   tEntire  sEntire  \n",
       "0      1.0     93.0  \n",
       "1      0.0    790.0  \n",
       "2      3.0   5119.0  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_processed = preprocess(\n",
    "    X_model, \n",
    "    [\n",
    "        cat_Featrues,\n",
    "        dist_GIT,\n",
    "        dist_VAT,\n",
    "        entire,\n",
    "        # array_divide(dist_GIT, entire), # rel_GIT\n",
    "        # array_divide(dist_VAT, entire[1:]), # rel_VAT\n",
    "    ]\n",
    ")\n",
    "X_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73973355",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total = pd.concat([X_processed,Y_model],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c62086ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=1\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "for col in ['cGIT', 'tGIT', 'sGIT', 'tVAT', 'sVAT', 'cEntire', 'tEntire','sEntire']:\n",
    "    x = Total[col]\n",
    "    y = Total['business']\n",
    "    plt.subplot(4,2,i)\n",
    "    i+=1\n",
    "    plt.scatter(x,y)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d979bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Total[Total['cGIT']>800].index\n",
    "b = Total[Total['tGIT']>500].index\n",
    "c = Total[Total['sGIT']>6000000].index\n",
    "\n",
    "d = Total[Total['tVAT']>700].index\n",
    "e = Total[Total['sVAT']>1000000].index\n",
    "\n",
    "f = Total[Total['cEntire']>5000].index\n",
    "g = Total[Total['tEntire']>3000].index\n",
    "h = Total[Total['sEntire']>5000000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "05954879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276935, 661865, 15127, 696728, 679738]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[276935, 661865, 15127, 696728, 679738] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [141]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m OL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(OL))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(OL)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mX_processed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOL\u001b[49m\u001b[43m,\u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4806\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   4807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   4808\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4816\u001b[0m ):\n\u001b[0;32m   4817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4952\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4956\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: '[276935, 661865, 15127, 696728, 679738] not found in axis'"
     ]
    }
   ],
   "source": [
    "OL = []\n",
    "for i in [a,b,c,d,e,f,g,h]:\n",
    "    for index in i:\n",
    "        OL.append(index)\n",
    "OL = list(set(OL))\n",
    "print(OL)\n",
    "X_processed.drop(OL,inplace=True)\n",
    "Y_model.drop(OL,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "674724ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a9639912264d7a98086f379506275e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=582, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=582\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.011710084780926661, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.011710084780926661\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.2716371375163257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2716371375163257\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9035769163814827, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9035769163814827\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=582, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=582\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.011710084780926661, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.011710084780926661\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.2716371375163257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2716371375163257\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9035769163814827, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9035769163814827\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=582, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=582\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.011710084780926661, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.011710084780926661\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.2716371375163257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2716371375163257\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9035769163814827, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9035769163814827\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=582, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=582\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.011710084780926661, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.011710084780926661\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.2716371375163257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2716371375163257\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9035769163814827, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9035769163814827\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=704, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=704\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.20372237748273395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.20372237748273395\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.2022077613655121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2022077613655121\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.425111339451517, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.425111339451517\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=704, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=704\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.20372237748273395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.20372237748273395\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.2022077613655121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2022077613655121\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.425111339451517, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.425111339451517\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=704, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=704\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.20372237748273395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.20372237748273395\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.2022077613655121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2022077613655121\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.425111339451517, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.425111339451517\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=704, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=704\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.20372237748273395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.20372237748273395\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.2022077613655121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2022077613655121\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.425111339451517, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.425111339451517\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=704, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=704\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.20372237748273395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.20372237748273395\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.2022077613655121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2022077613655121\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.425111339451517, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.425111339451517\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=836, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=836\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.689589398278062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.689589398278062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46876953353285844, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46876953353285844\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001028913084747022, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001028913084747022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=836, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=836\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.689589398278062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.689589398278062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46876953353285844, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46876953353285844\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001028913084747022, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001028913084747022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=836, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=836\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.689589398278062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.689589398278062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46876953353285844, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46876953353285844\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001028913084747022, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001028913084747022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=836, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=836\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.689589398278062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.689589398278062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46876953353285844, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46876953353285844\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001028913084747022, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001028913084747022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=836, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=836\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.689589398278062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.689589398278062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46876953353285844, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46876953353285844\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001028913084747022, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001028913084747022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=666, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=666\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014401648369176252, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014401648369176252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6558849005970908, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6558849005970908\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15244540926851988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15244540926851988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=666, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=666\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014401648369176252, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014401648369176252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6558849005970908, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6558849005970908\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15244540926851988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15244540926851988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=666, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=666\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014401648369176252, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014401648369176252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6558849005970908, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6558849005970908\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15244540926851988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15244540926851988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=666, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=666\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014401648369176252, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014401648369176252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6558849005970908, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6558849005970908\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15244540926851988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15244540926851988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=666, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=666\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014401648369176252, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014401648369176252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6558849005970908, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6558849005970908\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15244540926851988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15244540926851988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1536372732827927, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1536372732827927\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.45552448475248997, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45552448475248997\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.012181011703762215, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.012181011703762215\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1536372732827927, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1536372732827927\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.45552448475248997, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45552448475248997\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.012181011703762215, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.012181011703762215\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1536372732827927, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1536372732827927\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.45552448475248997, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45552448475248997\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.012181011703762215, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.012181011703762215\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1536372732827927, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1536372732827927\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.45552448475248997, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45552448475248997\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.012181011703762215, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.012181011703762215\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1536372732827927, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1536372732827927\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.45552448475248997, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45552448475248997\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.012181011703762215, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.012181011703762215\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=669, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19352484223647848, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19352484223647848\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6655222911750847, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6655222911750847\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0095500497661694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0095500497661694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=669, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19352484223647848, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19352484223647848\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6655222911750847, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6655222911750847\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0095500497661694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0095500497661694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=669, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19352484223647848, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19352484223647848\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6655222911750847, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6655222911750847\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0095500497661694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0095500497661694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=669, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19352484223647848, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19352484223647848\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6655222911750847, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6655222911750847\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0095500497661694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0095500497661694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=669, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19352484223647848, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19352484223647848\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6655222911750847, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6655222911750847\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0095500497661694, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0095500497661694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=502, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=502\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.011071115722728935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.011071115722728935\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24311475007543404, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24311475007543404\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.167086993658732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.167086993658732\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=502, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=502\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.011071115722728935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.011071115722728935\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24311475007543404, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24311475007543404\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.167086993658732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.167086993658732\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=502, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=502\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.011071115722728935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.011071115722728935\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24311475007543404, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24311475007543404\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.167086993658732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.167086993658732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=502, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=502\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.011071115722728935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.011071115722728935\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24311475007543404, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24311475007543404\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.167086993658732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.167086993658732\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=502, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=502\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.011071115722728935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.011071115722728935\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24311475007543404, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24311475007543404\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.167086993658732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.167086993658732\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=766, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=766\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.5328731735161432, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5328731735161432\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.23036315010800756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.23036315010800756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24812966852839397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24812966852839397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=766, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=766\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.5328731735161432, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5328731735161432\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.23036315010800756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.23036315010800756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24812966852839397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24812966852839397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=766, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=766\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.5328731735161432, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5328731735161432\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.23036315010800756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.23036315010800756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24812966852839397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24812966852839397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=766, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=766\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.5328731735161432, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5328731735161432\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.23036315010800756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.23036315010800756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24812966852839397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24812966852839397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=766, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=766\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.5328731735161432, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5328731735161432\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.23036315010800756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.23036315010800756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24812966852839397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24812966852839397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=613, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=613\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.020903373329595132, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.020903373329595132\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.49222998117783523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.49222998117783523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.011768513351131993, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011768513351131993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=613, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=613\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.020903373329595132, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.020903373329595132\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.49222998117783523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.49222998117783523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.011768513351131993, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011768513351131993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=613, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=613\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.020903373329595132, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.020903373329595132\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.49222998117783523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.49222998117783523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.011768513351131993, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011768513351131993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=613, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=613\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.020903373329595132, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.020903373329595132\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.49222998117783523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.49222998117783523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.011768513351131993, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011768513351131993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=613, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=613\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.020903373329595132, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.020903373329595132\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.49222998117783523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.49222998117783523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.011768513351131993, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011768513351131993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=780, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=780\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0371990529790659, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0371990529790659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.04434296073937394, subsample=1.0 will be ignored. Current value: bagging_fraction=0.04434296073937394\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9749230907377604, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9749230907377604\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=780, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=780\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0371990529790659, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0371990529790659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.04434296073937394, subsample=1.0 will be ignored. Current value: bagging_fraction=0.04434296073937394\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9749230907377604, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9749230907377604\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=780, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=780\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0371990529790659, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0371990529790659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.04434296073937394, subsample=1.0 will be ignored. Current value: bagging_fraction=0.04434296073937394\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9749230907377604, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9749230907377604\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=780, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=780\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0371990529790659, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0371990529790659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.04434296073937394, subsample=1.0 will be ignored. Current value: bagging_fraction=0.04434296073937394\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9749230907377604, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9749230907377604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=780, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=780\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0371990529790659, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0371990529790659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.04434296073937394, subsample=1.0 will be ignored. Current value: bagging_fraction=0.04434296073937394\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9749230907377604, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9749230907377604\n",
      "{'n_estimators': 194, 'max_depth': 11, 'learning_rate': 0.0290981935893862, 'num_leaves': 781, 'min_data_in_leaf': 502, 'max_bin': 280, 'lambda_l1': 0.011071115722728935, 'lambda_l2': 1.167086993658732, 'min_child_weight': 6, 'bagging_fraction': 0.24311475007543404, 'pos_bagging_fraction': 0.15799523917408742, 'neg_bagging_fraction': 0.6463318849401619}\n"
     ]
    }
   ],
   "source": [
    "#optimize by using optuna\n",
    "def optimizeLGBM(trial):\n",
    "    lgbm = LGBMClassifier(\n",
    "                          task = \"train\",\n",
    "                          objective = \"binary\", #cross-entropy\n",
    "#                           boosting = \"gdbt\", #rf\n",
    "                          n_estimators=trial.suggest_int('n_estimators',100,500),\n",
    "                          # to deal with overfitting, very important param\n",
    "                          max_depth = trial.suggest_int('max_depth',10,20),\n",
    "                          learning_rate = trial.suggest_float('learning_rate',0.02,0.1),\n",
    "                          num_leaves = trial.suggest_int('num_leaves',500,1000),\n",
    "                          min_data_in_leaf = trial.suggest_int('min_data_in_leaf',100,1000),\n",
    "                          metric = \"auc\",\n",
    "                          #if max_bin becomes small, the accuracy goes up\n",
    "                          max_bin = trial.suggest_int('max_bin',255,350),\n",
    "                          tree_learner = \"data\",\n",
    "                          lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-3, 10.0),\n",
    "                          lambda_l2 = trial.suggest_loguniform('lambda_l2', 1e-3, 10.0),\n",
    "                          # to deal with overfitting\n",
    "                          min_child_weight = trial.suggest_int('min_child_weight', 1, 10),\n",
    "                          random_state=100,\n",
    "                          #for bagging imbalanced\n",
    "                          bagging_fraction = trial.suggest_float('bagging_fraction', 0,1),\n",
    "                          pos_bagging_fraction = trial.suggest_float('pos_bagging_fraction', 0,1),\n",
    "                          neg_bagging_fraction = trial.suggest_float('neg_bagging_fraction', 0,1),\n",
    "                          categorical_feature = [0,1,2],\n",
    "        \n",
    "#                           is_unbalance = True\n",
    "                          class_weight={0: 1, 1: 14.291397}\n",
    "        \n",
    "#                           boosting = \"gdbt\", #rf\n",
    "#                           min_gain_to_split = ,\n",
    "#                           bagging_fraction = ,\n",
    "#                           early_stopping_round = ,\n",
    "\n",
    "\n",
    "    )\n",
    "    #cross validation K=5\n",
    "    score = cross_val_score(lgbm, X_processed, Y_model, cv=StratifiedKFold(n_splits=5, shuffle=True),\n",
    "                            scoring=\"roc_auc\")\n",
    "    return score.mean()\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "LGBM_study = optuna.create_study(direction='maximize')\n",
    "LGBM_study.optimize(optimizeLGBM, show_progress_bar=True, n_trials=10)\n",
    "\n",
    "# Print the best parameters\n",
    "print(LGBM_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2b5e99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model and fit\n",
    "optimized_LGBM = LGBMClassifier(task = \"predict\",\n",
    "                          objective = \"binary\", # cross-entropy\n",
    "#                           boosting = gdbt, #rf\n",
    "                          n_estimators=LGBM_study.best_params['n_estimators'],\n",
    "                          # to deal with overfitting, very important param\n",
    "                          max_depth = LGBM_study.best_params['max_depth'],\n",
    "                          learning_rate = LGBM_study.best_params['learning_rate'], # if it becomes 0.01(maybe?)the result proba becomes extremely small\n",
    "                          num_leaves = LGBM_study.best_params['num_leaves'],\n",
    "                          min_data_in_leaf = LGBM_study.best_params['min_data_in_leaf'],\n",
    "                          metric = \"auc\",\n",
    "                          #if max_bin becomes small, the accuracy goes up\n",
    "                          max_bin = LGBM_study.best_params['max_bin'],\n",
    "                          tree_learner = \"data\",\n",
    "                          lambda_l1 = LGBM_study.best_params['lambda_l1'],\n",
    "                          lambda_l2 = LGBM_study.best_params['lambda_l2'],\n",
    "                          # to deal with overfitting\n",
    "                          min_child_weight = LGBM_study.best_params['min_child_weight'], #LGBM_study.best_params['min_child_weight']\n",
    "                          random_state=100,\n",
    "                          bagging_fraction = LGBM_study.best_params['bagging_fraction'],\n",
    "                          pos_bagging_fraction = LGBM_study.best_params['pos_bagging_fraction'],\n",
    "                          neg_bagging_fraction = LGBM_study.best_params['pos_bagging_fraction'],\n",
    "#                           is_unbalance = True\n",
    "                          categorical_feature = [0,1,2],\n",
    "                          class_weight={0: 1, 1: 14.291397}\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "142fa419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=175, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=175\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6502663499503786, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6502663499503786\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.37148821105960206, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37148821105960206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.010169063863156927, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010169063863156927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=175, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=175\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6502663499503786, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6502663499503786\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.37148821105960206, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37148821105960206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.010169063863156927, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010169063863156927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=175, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=175\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6502663499503786, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6502663499503786\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.37148821105960206, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37148821105960206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.010169063863156927, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010169063863156927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=175, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=175\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6502663499503786, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6502663499503786\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.37148821105960206, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37148821105960206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.010169063863156927, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010169063863156927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=175, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=175\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6502663499503786, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6502663499503786\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.37148821105960206, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37148821105960206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.010169063863156927, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010169063863156927\n",
      "Average ROC AUC Score 0.8768152693484226\n",
      "Standard Deviation of ROC AUC Score 0.0013888767679042424\n",
      "[0.87730129 0.8747174  0.87899266 0.87630156 0.87676343]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate\n",
    "scores = cross_val_score(\n",
    "    optimized_LGBM, \n",
    "    X_processed, \n",
    "    Y_model, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True\n",
    "                       ,random_state=100\n",
    "                      ),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "print(\"Average ROC AUC Score\", np.mean(scores))\n",
    "print(\"Standard Deviation of ROC AUC Score\", np.std(scores))\n",
    "# Plot 5 scores in bar plot\n",
    "print(scores)\n",
    "plt.bar(list(map(lambda i: f\"Fold #{i}\", range(1, 6))), scores)\n",
    "\n",
    "#0.8753111530570191\n",
    "#0.8756329404995423\n",
    "#0.8780611462672319 when iterate 10 times\n",
    "#0.8786665892827801 when iterate 100 times\n",
    "#0.8786365497775094 -> trial 7 when iterate 10 times\n",
    "\n",
    "#0.8778765687408898 ???? why?\n",
    "#0.8778026069580795 when remove outlier\n",
    "#0.8783678297299062 when remove outlier + class_weight={0: 1, 1: 14.291397}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01560ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing...\n"
     ]
    }
   ],
   "source": [
    "print(\"Data preprocessing...\")\n",
    "X_exam = pd.read_csv('X_exam.csv')\n",
    "dist_GIT_exam = rangesum(\n",
    "    'GIT', \n",
    "    r\"202205[0-9]{2}\", \n",
    "    \"cts\", \n",
    "    equal_dist(31)\n",
    ")(X_exam)\n",
    "dist_VAT_exam = rangesum(\n",
    "    'VAT', \n",
    "    r\"20220[17](?:[01][0-9]|2[0-5])\", \n",
    "    \"ts\", \n",
    "    np.concatenate((equal_dist(25), equal_dist(25)))\n",
    ")(X_exam)\n",
    "entire_days = 31 + 29 + 31 + 30 + 31 + 30 + 31 + 25\n",
    "entire_exam = rangesum(\n",
    "    'Entire', \n",
    "    r\"2022[0-9]{4}\", \n",
    "    \"cts\", \n",
    "    equal_dist(entire_days)\n",
    ")(X_exam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2605e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_exam_processed = preprocess(\n",
    "    X_exam, \n",
    "    [\n",
    "        column(['age_code']),\n",
    "        one_hot_encode('gender'),\n",
    "        one_hot_encode('region_code'),\n",
    "        dist_GIT_exam,\n",
    "        dist_VAT_exam,\n",
    "        entire_exam\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60b43b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_code</th>\n",
       "      <th>gender_1</th>\n",
       "      <th>gender_2</th>\n",
       "      <th>region_code_0</th>\n",
       "      <th>region_code_1</th>\n",
       "      <th>region_code_2</th>\n",
       "      <th>region_code_4</th>\n",
       "      <th>region_code_5</th>\n",
       "      <th>region_code_6</th>\n",
       "      <th>region_code_7</th>\n",
       "      <th>...</th>\n",
       "      <th>region_code_17</th>\n",
       "      <th>region_code_18</th>\n",
       "      <th>cGIT</th>\n",
       "      <th>tGIT</th>\n",
       "      <th>sGIT</th>\n",
       "      <th>tVAT</th>\n",
       "      <th>sVAT</th>\n",
       "      <th>cEntire</th>\n",
       "      <th>tEntire</th>\n",
       "      <th>sEntire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008194</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.020179</td>\n",
       "      <td>0.013322</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.009199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_code  gender_1  gender_2  region_code_0  region_code_1  region_code_2  \\\n",
       "0  0.230769       0.0       1.0            0.0            0.0            0.0   \n",
       "1  0.692308       0.0       1.0            0.0            0.0            0.0   \n",
       "2  0.230769       0.0       1.0            0.0            0.0            1.0   \n",
       "3  0.538462       1.0       0.0            0.0            1.0            0.0   \n",
       "4  0.538462       0.0       1.0            0.0            0.0            1.0   \n",
       "\n",
       "   region_code_4  region_code_5  region_code_6  region_code_7  ...  \\\n",
       "0            1.0            0.0            0.0            0.0  ...   \n",
       "1            0.0            0.0            0.0            0.0  ...   \n",
       "2            0.0            0.0            0.0            0.0  ...   \n",
       "3            0.0            0.0            0.0            0.0  ...   \n",
       "4            0.0            0.0            0.0            0.0  ...   \n",
       "\n",
       "   region_code_17  region_code_18      cGIT  tGIT      sGIT      tVAT  \\\n",
       "0             0.0             0.0  0.008197   0.0  0.000195  0.004158   \n",
       "1             0.0             0.0  0.000000   0.0  0.000000  0.000000   \n",
       "2             0.0             0.0  0.008197   0.0  0.008194  0.004158   \n",
       "3             0.0             0.0  0.002049   0.0  0.000512  0.000000   \n",
       "4             0.0             0.0  0.000000   0.0  0.000000  0.002079   \n",
       "\n",
       "       sVAT   cEntire   tEntire   sEntire  \n",
       "0  0.003473  0.004441  0.000978  0.002425  \n",
       "1  0.000000  0.001943  0.000000  0.002560  \n",
       "2  0.020179  0.013322  0.003421  0.009199  \n",
       "3  0.000000  0.003608  0.000000  0.002000  \n",
       "4  0.003622  0.000278  0.000489  0.000997  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_exam_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1a29356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing...\n",
      "[1]\ttraining's auc: 0.868478\tvalid_1's auc: 0.867066\n",
      "[2]\ttraining's auc: 0.872429\tvalid_1's auc: 0.870279\n",
      "[3]\ttraining's auc: 0.874213\tvalid_1's auc: 0.871943\n",
      "[4]\ttraining's auc: 0.874975\tvalid_1's auc: 0.872607\n",
      "[5]\ttraining's auc: 0.87555\tvalid_1's auc: 0.872993\n",
      "[6]\ttraining's auc: 0.876325\tvalid_1's auc: 0.873369\n",
      "[7]\ttraining's auc: 0.876955\tvalid_1's auc: 0.873786\n",
      "[8]\ttraining's auc: 0.877613\tvalid_1's auc: 0.874178\n",
      "[9]\ttraining's auc: 0.878087\tvalid_1's auc: 0.874455\n",
      "[10]\ttraining's auc: 0.878465\tvalid_1's auc: 0.874651\n",
      "[11]\ttraining's auc: 0.878731\tvalid_1's auc: 0.874792\n",
      "[12]\ttraining's auc: 0.879131\tvalid_1's auc: 0.874977\n",
      "[13]\ttraining's auc: 0.879422\tvalid_1's auc: 0.875074\n",
      "[14]\ttraining's auc: 0.879836\tvalid_1's auc: 0.875269\n",
      "[15]\ttraining's auc: 0.880107\tvalid_1's auc: 0.875364\n",
      "[16]\ttraining's auc: 0.880415\tvalid_1's auc: 0.875513\n",
      "[17]\ttraining's auc: 0.880711\tvalid_1's auc: 0.875626\n",
      "[18]\ttraining's auc: 0.880869\tvalid_1's auc: 0.875599\n",
      "[19]\ttraining's auc: 0.88112\tvalid_1's auc: 0.875612\n",
      "[20]\ttraining's auc: 0.881381\tvalid_1's auc: 0.875722\n",
      "[21]\ttraining's auc: 0.881699\tvalid_1's auc: 0.875835\n",
      "[22]\ttraining's auc: 0.881968\tvalid_1's auc: 0.875879\n",
      "[23]\ttraining's auc: 0.882275\tvalid_1's auc: 0.875948\n",
      "[24]\ttraining's auc: 0.882545\tvalid_1's auc: 0.876023\n",
      "[25]\ttraining's auc: 0.882818\tvalid_1's auc: 0.876089\n",
      "[26]\ttraining's auc: 0.883033\tvalid_1's auc: 0.87613\n",
      "[27]\ttraining's auc: 0.883254\tvalid_1's auc: 0.876136\n",
      "[28]\ttraining's auc: 0.883456\tvalid_1's auc: 0.876183\n",
      "[29]\ttraining's auc: 0.883698\tvalid_1's auc: 0.876203\n",
      "[30]\ttraining's auc: 0.883884\tvalid_1's auc: 0.87623\n",
      "[31]\ttraining's auc: 0.884066\tvalid_1's auc: 0.876263\n",
      "[32]\ttraining's auc: 0.884273\tvalid_1's auc: 0.876332\n",
      "[33]\ttraining's auc: 0.884501\tvalid_1's auc: 0.876379\n",
      "[34]\ttraining's auc: 0.884783\tvalid_1's auc: 0.876448\n",
      "[35]\ttraining's auc: 0.885003\tvalid_1's auc: 0.876476\n",
      "[36]\ttraining's auc: 0.88522\tvalid_1's auc: 0.876528\n",
      "[37]\ttraining's auc: 0.885462\tvalid_1's auc: 0.876577\n",
      "[38]\ttraining's auc: 0.885778\tvalid_1's auc: 0.876614\n",
      "[39]\ttraining's auc: 0.88598\tvalid_1's auc: 0.876635\n",
      "[40]\ttraining's auc: 0.886317\tvalid_1's auc: 0.876675\n",
      "[41]\ttraining's auc: 0.886484\tvalid_1's auc: 0.876697\n",
      "[42]\ttraining's auc: 0.886663\tvalid_1's auc: 0.876696\n",
      "[43]\ttraining's auc: 0.886952\tvalid_1's auc: 0.876707\n",
      "[44]\ttraining's auc: 0.887171\tvalid_1's auc: 0.876741\n",
      "[45]\ttraining's auc: 0.887349\tvalid_1's auc: 0.876766\n",
      "[46]\ttraining's auc: 0.887617\tvalid_1's auc: 0.876807\n",
      "[47]\ttraining's auc: 0.887855\tvalid_1's auc: 0.876849\n",
      "[48]\ttraining's auc: 0.888072\tvalid_1's auc: 0.876884\n",
      "[49]\ttraining's auc: 0.888286\tvalid_1's auc: 0.876928\n",
      "[50]\ttraining's auc: 0.888475\tvalid_1's auc: 0.876943\n",
      "[51]\ttraining's auc: 0.88871\tvalid_1's auc: 0.876975\n",
      "[52]\ttraining's auc: 0.888863\tvalid_1's auc: 0.877034\n",
      "[53]\ttraining's auc: 0.889122\tvalid_1's auc: 0.877093\n",
      "[54]\ttraining's auc: 0.889309\tvalid_1's auc: 0.877114\n",
      "[55]\ttraining's auc: 0.889458\tvalid_1's auc: 0.877125\n",
      "[56]\ttraining's auc: 0.889642\tvalid_1's auc: 0.87714\n",
      "[57]\ttraining's auc: 0.889782\tvalid_1's auc: 0.877153\n",
      "[58]\ttraining's auc: 0.889969\tvalid_1's auc: 0.877205\n",
      "[59]\ttraining's auc: 0.890091\tvalid_1's auc: 0.877254\n",
      "[60]\ttraining's auc: 0.890204\tvalid_1's auc: 0.877271\n",
      "[61]\ttraining's auc: 0.890327\tvalid_1's auc: 0.877289\n",
      "[62]\ttraining's auc: 0.890545\tvalid_1's auc: 0.877366\n",
      "[63]\ttraining's auc: 0.890714\tvalid_1's auc: 0.877409\n",
      "[64]\ttraining's auc: 0.890867\tvalid_1's auc: 0.87744\n",
      "[65]\ttraining's auc: 0.890979\tvalid_1's auc: 0.877473\n",
      "[66]\ttraining's auc: 0.891104\tvalid_1's auc: 0.877498\n",
      "[67]\ttraining's auc: 0.891259\tvalid_1's auc: 0.877536\n",
      "[68]\ttraining's auc: 0.891426\tvalid_1's auc: 0.877573\n",
      "[69]\ttraining's auc: 0.89154\tvalid_1's auc: 0.877613\n",
      "[70]\ttraining's auc: 0.891612\tvalid_1's auc: 0.877635\n",
      "[71]\ttraining's auc: 0.891781\tvalid_1's auc: 0.87765\n",
      "[72]\ttraining's auc: 0.891948\tvalid_1's auc: 0.877697\n",
      "[73]\ttraining's auc: 0.892057\tvalid_1's auc: 0.877709\n",
      "[74]\ttraining's auc: 0.892107\tvalid_1's auc: 0.877713\n",
      "[75]\ttraining's auc: 0.892272\tvalid_1's auc: 0.877761\n",
      "[76]\ttraining's auc: 0.892375\tvalid_1's auc: 0.877804\n",
      "[77]\ttraining's auc: 0.892526\tvalid_1's auc: 0.877851\n",
      "[78]\ttraining's auc: 0.892691\tvalid_1's auc: 0.877884\n",
      "[79]\ttraining's auc: 0.892801\tvalid_1's auc: 0.877876\n",
      "[80]\ttraining's auc: 0.892994\tvalid_1's auc: 0.877889\n",
      "[81]\ttraining's auc: 0.893049\tvalid_1's auc: 0.877921\n",
      "[82]\ttraining's auc: 0.893167\tvalid_1's auc: 0.877933\n",
      "[83]\ttraining's auc: 0.893272\tvalid_1's auc: 0.877945\n",
      "[84]\ttraining's auc: 0.893411\tvalid_1's auc: 0.877984\n",
      "[85]\ttraining's auc: 0.893517\tvalid_1's auc: 0.878025\n",
      "[86]\ttraining's auc: 0.893658\tvalid_1's auc: 0.878044\n",
      "[87]\ttraining's auc: 0.893774\tvalid_1's auc: 0.878077\n",
      "[88]\ttraining's auc: 0.893868\tvalid_1's auc: 0.878066\n",
      "[89]\ttraining's auc: 0.893916\tvalid_1's auc: 0.878076\n",
      "[90]\ttraining's auc: 0.894045\tvalid_1's auc: 0.878105\n",
      "[91]\ttraining's auc: 0.894171\tvalid_1's auc: 0.878146\n",
      "[92]\ttraining's auc: 0.894318\tvalid_1's auc: 0.87817\n",
      "[93]\ttraining's auc: 0.894421\tvalid_1's auc: 0.878198\n",
      "[94]\ttraining's auc: 0.894597\tvalid_1's auc: 0.8782\n",
      "[95]\ttraining's auc: 0.894758\tvalid_1's auc: 0.878211\n",
      "[96]\ttraining's auc: 0.894886\tvalid_1's auc: 0.87824\n",
      "[97]\ttraining's auc: 0.894983\tvalid_1's auc: 0.878249\n",
      "[98]\ttraining's auc: 0.895077\tvalid_1's auc: 0.878247\n",
      "[99]\ttraining's auc: 0.895115\tvalid_1's auc: 0.878265\n",
      "[100]\ttraining's auc: 0.895231\tvalid_1's auc: 0.878252\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[1]\ttraining's auc: 0.868461\tvalid_1's auc: 0.865744\n",
      "[2]\ttraining's auc: 0.872381\tvalid_1's auc: 0.869007\n",
      "[3]\ttraining's auc: 0.874294\tvalid_1's auc: 0.870506\n",
      "[4]\ttraining's auc: 0.875212\tvalid_1's auc: 0.87127\n",
      "[5]\ttraining's auc: 0.875659\tvalid_1's auc: 0.87144\n",
      "[6]\ttraining's auc: 0.876403\tvalid_1's auc: 0.872001\n",
      "[7]\ttraining's auc: 0.877069\tvalid_1's auc: 0.872344\n",
      "[8]\ttraining's auc: 0.877667\tvalid_1's auc: 0.872674\n",
      "[9]\ttraining's auc: 0.878196\tvalid_1's auc: 0.872942\n",
      "[10]\ttraining's auc: 0.878575\tvalid_1's auc: 0.873066\n",
      "[11]\ttraining's auc: 0.879051\tvalid_1's auc: 0.873331\n",
      "[12]\ttraining's auc: 0.879431\tvalid_1's auc: 0.873465\n",
      "[13]\ttraining's auc: 0.879834\tvalid_1's auc: 0.873594\n",
      "[14]\ttraining's auc: 0.880178\tvalid_1's auc: 0.873701\n",
      "[15]\ttraining's auc: 0.880548\tvalid_1's auc: 0.873849\n",
      "[16]\ttraining's auc: 0.880735\tvalid_1's auc: 0.873948\n",
      "[17]\ttraining's auc: 0.881019\tvalid_1's auc: 0.873958\n",
      "[18]\ttraining's auc: 0.881368\tvalid_1's auc: 0.874053\n",
      "[19]\ttraining's auc: 0.881608\tvalid_1's auc: 0.87412\n",
      "[20]\ttraining's auc: 0.881924\tvalid_1's auc: 0.874164\n",
      "[21]\ttraining's auc: 0.882223\tvalid_1's auc: 0.87421\n",
      "[22]\ttraining's auc: 0.882462\tvalid_1's auc: 0.874262\n",
      "[23]\ttraining's auc: 0.882687\tvalid_1's auc: 0.874328\n",
      "[24]\ttraining's auc: 0.882905\tvalid_1's auc: 0.874446\n",
      "[25]\ttraining's auc: 0.883192\tvalid_1's auc: 0.874492\n",
      "[26]\ttraining's auc: 0.883473\tvalid_1's auc: 0.874556\n",
      "[27]\ttraining's auc: 0.883674\tvalid_1's auc: 0.874562\n",
      "[28]\ttraining's auc: 0.883864\tvalid_1's auc: 0.874609\n",
      "[29]\ttraining's auc: 0.884064\tvalid_1's auc: 0.874677\n",
      "[30]\ttraining's auc: 0.884305\tvalid_1's auc: 0.874718\n",
      "[31]\ttraining's auc: 0.884537\tvalid_1's auc: 0.874769\n",
      "[32]\ttraining's auc: 0.884793\tvalid_1's auc: 0.874771\n",
      "[33]\ttraining's auc: 0.885145\tvalid_1's auc: 0.874838\n",
      "[34]\ttraining's auc: 0.885308\tvalid_1's auc: 0.874864\n",
      "[35]\ttraining's auc: 0.885604\tvalid_1's auc: 0.874877\n",
      "[36]\ttraining's auc: 0.885823\tvalid_1's auc: 0.874871\n",
      "[37]\ttraining's auc: 0.88609\tvalid_1's auc: 0.874921\n",
      "[38]\ttraining's auc: 0.8863\tvalid_1's auc: 0.874952\n",
      "[39]\ttraining's auc: 0.886565\tvalid_1's auc: 0.875018\n",
      "[40]\ttraining's auc: 0.886738\tvalid_1's auc: 0.875069\n",
      "[41]\ttraining's auc: 0.886969\tvalid_1's auc: 0.875128\n",
      "[42]\ttraining's auc: 0.887213\tvalid_1's auc: 0.87519\n",
      "[43]\ttraining's auc: 0.887465\tvalid_1's auc: 0.87526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44]\ttraining's auc: 0.887656\tvalid_1's auc: 0.875275\n",
      "[45]\ttraining's auc: 0.887867\tvalid_1's auc: 0.875287\n",
      "[46]\ttraining's auc: 0.888039\tvalid_1's auc: 0.875309\n",
      "[47]\ttraining's auc: 0.888177\tvalid_1's auc: 0.875311\n",
      "[48]\ttraining's auc: 0.888405\tvalid_1's auc: 0.875323\n",
      "[49]\ttraining's auc: 0.888566\tvalid_1's auc: 0.875355\n",
      "[50]\ttraining's auc: 0.888754\tvalid_1's auc: 0.875397\n",
      "[51]\ttraining's auc: 0.888978\tvalid_1's auc: 0.875426\n",
      "[52]\ttraining's auc: 0.88915\tvalid_1's auc: 0.87544\n",
      "[53]\ttraining's auc: 0.88938\tvalid_1's auc: 0.87547\n",
      "[54]\ttraining's auc: 0.889559\tvalid_1's auc: 0.875482\n",
      "[55]\ttraining's auc: 0.889743\tvalid_1's auc: 0.87549\n",
      "[56]\ttraining's auc: 0.889907\tvalid_1's auc: 0.875504\n",
      "[57]\ttraining's auc: 0.890071\tvalid_1's auc: 0.875524\n",
      "[58]\ttraining's auc: 0.890279\tvalid_1's auc: 0.875563\n",
      "[59]\ttraining's auc: 0.890428\tvalid_1's auc: 0.875581\n",
      "[60]\ttraining's auc: 0.890557\tvalid_1's auc: 0.875592\n",
      "[61]\ttraining's auc: 0.890698\tvalid_1's auc: 0.875592\n",
      "[62]\ttraining's auc: 0.890841\tvalid_1's auc: 0.875635\n",
      "[63]\ttraining's auc: 0.891024\tvalid_1's auc: 0.875641\n",
      "[64]\ttraining's auc: 0.891148\tvalid_1's auc: 0.875662\n",
      "[65]\ttraining's auc: 0.891298\tvalid_1's auc: 0.875639\n",
      "[66]\ttraining's auc: 0.891377\tvalid_1's auc: 0.875646\n",
      "[67]\ttraining's auc: 0.891489\tvalid_1's auc: 0.875671\n",
      "[68]\ttraining's auc: 0.891629\tvalid_1's auc: 0.87569\n",
      "[69]\ttraining's auc: 0.89178\tvalid_1's auc: 0.875737\n",
      "[70]\ttraining's auc: 0.891883\tvalid_1's auc: 0.87576\n",
      "[71]\ttraining's auc: 0.892029\tvalid_1's auc: 0.875823\n",
      "[72]\ttraining's auc: 0.892128\tvalid_1's auc: 0.875826\n",
      "[73]\ttraining's auc: 0.892244\tvalid_1's auc: 0.875837\n",
      "[74]\ttraining's auc: 0.892374\tvalid_1's auc: 0.875861\n",
      "[75]\ttraining's auc: 0.892473\tvalid_1's auc: 0.87586\n",
      "[76]\ttraining's auc: 0.892554\tvalid_1's auc: 0.875855\n",
      "[77]\ttraining's auc: 0.89266\tvalid_1's auc: 0.875858\n",
      "[78]\ttraining's auc: 0.892741\tvalid_1's auc: 0.875874\n",
      "[79]\ttraining's auc: 0.892891\tvalid_1's auc: 0.875868\n",
      "[80]\ttraining's auc: 0.893005\tvalid_1's auc: 0.875879\n",
      "[81]\ttraining's auc: 0.89313\tvalid_1's auc: 0.875941\n",
      "[82]\ttraining's auc: 0.89322\tvalid_1's auc: 0.875952\n",
      "[83]\ttraining's auc: 0.893316\tvalid_1's auc: 0.875981\n",
      "[84]\ttraining's auc: 0.893452\tvalid_1's auc: 0.87601\n",
      "[85]\ttraining's auc: 0.893556\tvalid_1's auc: 0.876052\n",
      "[86]\ttraining's auc: 0.893635\tvalid_1's auc: 0.87608\n",
      "[87]\ttraining's auc: 0.893765\tvalid_1's auc: 0.876046\n",
      "[88]\ttraining's auc: 0.893889\tvalid_1's auc: 0.876055\n",
      "[89]\ttraining's auc: 0.894041\tvalid_1's auc: 0.876102\n",
      "[90]\ttraining's auc: 0.894151\tvalid_1's auc: 0.876133\n",
      "[91]\ttraining's auc: 0.894281\tvalid_1's auc: 0.876134\n",
      "[92]\ttraining's auc: 0.894397\tvalid_1's auc: 0.876156\n",
      "[93]\ttraining's auc: 0.894546\tvalid_1's auc: 0.87616\n",
      "[94]\ttraining's auc: 0.894645\tvalid_1's auc: 0.876162\n",
      "[95]\ttraining's auc: 0.894886\tvalid_1's auc: 0.876215\n",
      "[96]\ttraining's auc: 0.895003\tvalid_1's auc: 0.876205\n",
      "[97]\ttraining's auc: 0.895053\tvalid_1's auc: 0.876205\n",
      "[98]\ttraining's auc: 0.895128\tvalid_1's auc: 0.876235\n",
      "[99]\ttraining's auc: 0.895206\tvalid_1's auc: 0.876214\n",
      "[100]\ttraining's auc: 0.895402\tvalid_1's auc: 0.876185\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[1]\ttraining's auc: 0.868276\tvalid_1's auc: 0.86545\n",
      "[2]\ttraining's auc: 0.872461\tvalid_1's auc: 0.869581\n",
      "[3]\ttraining's auc: 0.873992\tvalid_1's auc: 0.871078\n",
      "[4]\ttraining's auc: 0.875139\tvalid_1's auc: 0.872204\n",
      "[5]\ttraining's auc: 0.87596\tvalid_1's auc: 0.872915\n",
      "[6]\ttraining's auc: 0.876481\tvalid_1's auc: 0.873292\n",
      "[7]\ttraining's auc: 0.876925\tvalid_1's auc: 0.873492\n",
      "[8]\ttraining's auc: 0.877378\tvalid_1's auc: 0.873695\n",
      "[9]\ttraining's auc: 0.877927\tvalid_1's auc: 0.874105\n",
      "[10]\ttraining's auc: 0.878433\tvalid_1's auc: 0.874437\n",
      "[11]\ttraining's auc: 0.87883\tvalid_1's auc: 0.874658\n",
      "[12]\ttraining's auc: 0.879107\tvalid_1's auc: 0.874811\n",
      "[13]\ttraining's auc: 0.879532\tvalid_1's auc: 0.875019\n",
      "[14]\ttraining's auc: 0.879909\tvalid_1's auc: 0.875205\n",
      "[15]\ttraining's auc: 0.880262\tvalid_1's auc: 0.875406\n",
      "[16]\ttraining's auc: 0.88054\tvalid_1's auc: 0.875541\n",
      "[17]\ttraining's auc: 0.880897\tvalid_1's auc: 0.875636\n",
      "[18]\ttraining's auc: 0.881227\tvalid_1's auc: 0.875727\n",
      "[19]\ttraining's auc: 0.881532\tvalid_1's auc: 0.875852\n",
      "[20]\ttraining's auc: 0.881799\tvalid_1's auc: 0.875885\n",
      "[21]\ttraining's auc: 0.882056\tvalid_1's auc: 0.876\n",
      "[22]\ttraining's auc: 0.882327\tvalid_1's auc: 0.876078\n",
      "[23]\ttraining's auc: 0.882618\tvalid_1's auc: 0.876139\n",
      "[24]\ttraining's auc: 0.882944\tvalid_1's auc: 0.876266\n",
      "[25]\ttraining's auc: 0.883226\tvalid_1's auc: 0.876337\n",
      "[26]\ttraining's auc: 0.883493\tvalid_1's auc: 0.876378\n",
      "[27]\ttraining's auc: 0.883688\tvalid_1's auc: 0.876424\n",
      "[28]\ttraining's auc: 0.883976\tvalid_1's auc: 0.876479\n",
      "[29]\ttraining's auc: 0.884243\tvalid_1's auc: 0.876534\n",
      "[30]\ttraining's auc: 0.88449\tvalid_1's auc: 0.876592\n",
      "[31]\ttraining's auc: 0.884688\tvalid_1's auc: 0.87665\n",
      "[32]\ttraining's auc: 0.884847\tvalid_1's auc: 0.876699\n",
      "[33]\ttraining's auc: 0.885064\tvalid_1's auc: 0.876756\n",
      "[34]\ttraining's auc: 0.885282\tvalid_1's auc: 0.876814\n",
      "[35]\ttraining's auc: 0.885495\tvalid_1's auc: 0.876844\n",
      "[36]\ttraining's auc: 0.885712\tvalid_1's auc: 0.876847\n",
      "[37]\ttraining's auc: 0.885935\tvalid_1's auc: 0.876866\n",
      "[38]\ttraining's auc: 0.886164\tvalid_1's auc: 0.876845\n",
      "[39]\ttraining's auc: 0.886341\tvalid_1's auc: 0.876887\n",
      "[40]\ttraining's auc: 0.886569\tvalid_1's auc: 0.876922\n",
      "[41]\ttraining's auc: 0.886833\tvalid_1's auc: 0.876958\n",
      "[42]\ttraining's auc: 0.887041\tvalid_1's auc: 0.876954\n",
      "[43]\ttraining's auc: 0.88723\tvalid_1's auc: 0.876996\n",
      "[44]\ttraining's auc: 0.887486\tvalid_1's auc: 0.876997\n",
      "[45]\ttraining's auc: 0.887721\tvalid_1's auc: 0.877022\n",
      "[46]\ttraining's auc: 0.887893\tvalid_1's auc: 0.877015\n",
      "[47]\ttraining's auc: 0.888097\tvalid_1's auc: 0.877053\n",
      "[48]\ttraining's auc: 0.888317\tvalid_1's auc: 0.877012\n",
      "[49]\ttraining's auc: 0.88854\tvalid_1's auc: 0.87699\n",
      "[50]\ttraining's auc: 0.888765\tvalid_1's auc: 0.877015\n",
      "[51]\ttraining's auc: 0.888995\tvalid_1's auc: 0.877058\n",
      "[52]\ttraining's auc: 0.889151\tvalid_1's auc: 0.87705\n",
      "[53]\ttraining's auc: 0.889392\tvalid_1's auc: 0.877064\n",
      "[54]\ttraining's auc: 0.889545\tvalid_1's auc: 0.877097\n",
      "[55]\ttraining's auc: 0.889727\tvalid_1's auc: 0.877134\n",
      "[56]\ttraining's auc: 0.88988\tvalid_1's auc: 0.877168\n",
      "[57]\ttraining's auc: 0.890033\tvalid_1's auc: 0.877167\n",
      "[58]\ttraining's auc: 0.890168\tvalid_1's auc: 0.877173\n",
      "[59]\ttraining's auc: 0.890374\tvalid_1's auc: 0.877198\n",
      "[60]\ttraining's auc: 0.890612\tvalid_1's auc: 0.877237\n",
      "[61]\ttraining's auc: 0.890801\tvalid_1's auc: 0.877261\n",
      "[62]\ttraining's auc: 0.89098\tvalid_1's auc: 0.877293\n",
      "[63]\ttraining's auc: 0.891155\tvalid_1's auc: 0.877342\n",
      "[64]\ttraining's auc: 0.891318\tvalid_1's auc: 0.877372\n",
      "[65]\ttraining's auc: 0.891397\tvalid_1's auc: 0.877397\n",
      "[66]\ttraining's auc: 0.89161\tvalid_1's auc: 0.877414\n",
      "[67]\ttraining's auc: 0.891769\tvalid_1's auc: 0.877452\n",
      "[68]\ttraining's auc: 0.891962\tvalid_1's auc: 0.877457\n",
      "[69]\ttraining's auc: 0.892123\tvalid_1's auc: 0.877497\n",
      "[70]\ttraining's auc: 0.892181\tvalid_1's auc: 0.877515\n",
      "[71]\ttraining's auc: 0.892284\tvalid_1's auc: 0.877538\n",
      "[72]\ttraining's auc: 0.892359\tvalid_1's auc: 0.877566\n",
      "[73]\ttraining's auc: 0.892465\tvalid_1's auc: 0.877554\n",
      "[74]\ttraining's auc: 0.89253\tvalid_1's auc: 0.877544\n",
      "[75]\ttraining's auc: 0.892625\tvalid_1's auc: 0.877572\n",
      "[76]\ttraining's auc: 0.892677\tvalid_1's auc: 0.877583\n",
      "[77]\ttraining's auc: 0.892768\tvalid_1's auc: 0.877605\n",
      "[78]\ttraining's auc: 0.89292\tvalid_1's auc: 0.877657\n",
      "[79]\ttraining's auc: 0.893016\tvalid_1's auc: 0.877665\n",
      "[80]\ttraining's auc: 0.893131\tvalid_1's auc: 0.877689\n",
      "[81]\ttraining's auc: 0.893217\tvalid_1's auc: 0.877708\n",
      "[82]\ttraining's auc: 0.893373\tvalid_1's auc: 0.877754\n",
      "[83]\ttraining's auc: 0.893425\tvalid_1's auc: 0.877764\n",
      "[84]\ttraining's auc: 0.893542\tvalid_1's auc: 0.877757\n",
      "[85]\ttraining's auc: 0.893652\tvalid_1's auc: 0.877774\n",
      "[86]\ttraining's auc: 0.893785\tvalid_1's auc: 0.877814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87]\ttraining's auc: 0.893908\tvalid_1's auc: 0.877829\n",
      "[88]\ttraining's auc: 0.893973\tvalid_1's auc: 0.87784\n",
      "[89]\ttraining's auc: 0.894083\tvalid_1's auc: 0.877864\n",
      "[90]\ttraining's auc: 0.894267\tvalid_1's auc: 0.877909\n",
      "[91]\ttraining's auc: 0.894298\tvalid_1's auc: 0.877918\n",
      "[92]\ttraining's auc: 0.894409\tvalid_1's auc: 0.877919\n",
      "[93]\ttraining's auc: 0.894521\tvalid_1's auc: 0.87793\n",
      "[94]\ttraining's auc: 0.894606\tvalid_1's auc: 0.877964\n",
      "[95]\ttraining's auc: 0.894752\tvalid_1's auc: 0.877987\n",
      "[96]\ttraining's auc: 0.894941\tvalid_1's auc: 0.878039\n",
      "[97]\ttraining's auc: 0.895099\tvalid_1's auc: 0.878045\n",
      "[98]\ttraining's auc: 0.895201\tvalid_1's auc: 0.878044\n",
      "[99]\ttraining's auc: 0.895346\tvalid_1's auc: 0.878049\n",
      "[100]\ttraining's auc: 0.89548\tvalid_1's auc: 0.87805\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[1]\ttraining's auc: 0.867773\tvalid_1's auc: 0.865052\n",
      "[2]\ttraining's auc: 0.871906\tvalid_1's auc: 0.868813\n",
      "[3]\ttraining's auc: 0.873961\tvalid_1's auc: 0.871181\n",
      "[4]\ttraining's auc: 0.875071\tvalid_1's auc: 0.871663\n",
      "[5]\ttraining's auc: 0.876037\tvalid_1's auc: 0.872479\n",
      "[6]\ttraining's auc: 0.876711\tvalid_1's auc: 0.872986\n",
      "[7]\ttraining's auc: 0.877165\tvalid_1's auc: 0.87319\n",
      "[8]\ttraining's auc: 0.877603\tvalid_1's auc: 0.87345\n",
      "[9]\ttraining's auc: 0.878222\tvalid_1's auc: 0.873765\n",
      "[10]\ttraining's auc: 0.878676\tvalid_1's auc: 0.873897\n",
      "[11]\ttraining's auc: 0.879087\tvalid_1's auc: 0.874044\n",
      "[12]\ttraining's auc: 0.879369\tvalid_1's auc: 0.874193\n",
      "[13]\ttraining's auc: 0.879704\tvalid_1's auc: 0.874326\n",
      "[14]\ttraining's auc: 0.880004\tvalid_1's auc: 0.874449\n",
      "[15]\ttraining's auc: 0.880487\tvalid_1's auc: 0.874697\n",
      "[16]\ttraining's auc: 0.880809\tvalid_1's auc: 0.874858\n",
      "[17]\ttraining's auc: 0.881124\tvalid_1's auc: 0.87493\n",
      "[18]\ttraining's auc: 0.881371\tvalid_1's auc: 0.87507\n",
      "[19]\ttraining's auc: 0.881613\tvalid_1's auc: 0.875133\n",
      "[20]\ttraining's auc: 0.881939\tvalid_1's auc: 0.875194\n",
      "[21]\ttraining's auc: 0.882214\tvalid_1's auc: 0.875277\n",
      "[22]\ttraining's auc: 0.882464\tvalid_1's auc: 0.87531\n",
      "[23]\ttraining's auc: 0.882714\tvalid_1's auc: 0.875361\n",
      "[24]\ttraining's auc: 0.882984\tvalid_1's auc: 0.875439\n",
      "[25]\ttraining's auc: 0.883211\tvalid_1's auc: 0.875407\n",
      "[26]\ttraining's auc: 0.883446\tvalid_1's auc: 0.875408\n",
      "[27]\ttraining's auc: 0.883712\tvalid_1's auc: 0.875468\n",
      "[28]\ttraining's auc: 0.883976\tvalid_1's auc: 0.875502\n",
      "[29]\ttraining's auc: 0.884202\tvalid_1's auc: 0.875543\n",
      "[30]\ttraining's auc: 0.884474\tvalid_1's auc: 0.875555\n",
      "[31]\ttraining's auc: 0.884683\tvalid_1's auc: 0.875555\n",
      "[32]\ttraining's auc: 0.884934\tvalid_1's auc: 0.875614\n",
      "[33]\ttraining's auc: 0.885165\tvalid_1's auc: 0.875618\n",
      "[34]\ttraining's auc: 0.885431\tvalid_1's auc: 0.875621\n",
      "[35]\ttraining's auc: 0.885628\tvalid_1's auc: 0.875629\n",
      "[36]\ttraining's auc: 0.885859\tvalid_1's auc: 0.875686\n",
      "[37]\ttraining's auc: 0.886073\tvalid_1's auc: 0.875747\n",
      "[38]\ttraining's auc: 0.886272\tvalid_1's auc: 0.875759\n",
      "[39]\ttraining's auc: 0.886433\tvalid_1's auc: 0.875788\n",
      "[40]\ttraining's auc: 0.886688\tvalid_1's auc: 0.87585\n",
      "[41]\ttraining's auc: 0.886872\tvalid_1's auc: 0.875849\n",
      "[42]\ttraining's auc: 0.887086\tvalid_1's auc: 0.875849\n",
      "[43]\ttraining's auc: 0.887386\tvalid_1's auc: 0.875952\n",
      "[44]\ttraining's auc: 0.887549\tvalid_1's auc: 0.876005\n",
      "[45]\ttraining's auc: 0.887787\tvalid_1's auc: 0.876022\n",
      "[46]\ttraining's auc: 0.88793\tvalid_1's auc: 0.876036\n",
      "[47]\ttraining's auc: 0.88821\tvalid_1's auc: 0.876081\n",
      "[48]\ttraining's auc: 0.888448\tvalid_1's auc: 0.876099\n",
      "[49]\ttraining's auc: 0.888635\tvalid_1's auc: 0.876114\n",
      "[50]\ttraining's auc: 0.888808\tvalid_1's auc: 0.876132\n",
      "[51]\ttraining's auc: 0.88904\tvalid_1's auc: 0.87618\n",
      "[52]\ttraining's auc: 0.889204\tvalid_1's auc: 0.876214\n",
      "[53]\ttraining's auc: 0.889435\tvalid_1's auc: 0.876254\n",
      "[54]\ttraining's auc: 0.889639\tvalid_1's auc: 0.876274\n",
      "[55]\ttraining's auc: 0.889815\tvalid_1's auc: 0.876294\n",
      "[56]\ttraining's auc: 0.889975\tvalid_1's auc: 0.876327\n",
      "[57]\ttraining's auc: 0.890132\tvalid_1's auc: 0.876347\n",
      "[58]\ttraining's auc: 0.890302\tvalid_1's auc: 0.876359\n",
      "[59]\ttraining's auc: 0.890535\tvalid_1's auc: 0.876402\n",
      "[60]\ttraining's auc: 0.890684\tvalid_1's auc: 0.876412\n",
      "[61]\ttraining's auc: 0.890849\tvalid_1's auc: 0.876438\n",
      "[62]\ttraining's auc: 0.890977\tvalid_1's auc: 0.876438\n",
      "[63]\ttraining's auc: 0.891132\tvalid_1's auc: 0.876435\n",
      "[64]\ttraining's auc: 0.891319\tvalid_1's auc: 0.876467\n",
      "[65]\ttraining's auc: 0.891441\tvalid_1's auc: 0.876508\n",
      "[66]\ttraining's auc: 0.891604\tvalid_1's auc: 0.876535\n",
      "[67]\ttraining's auc: 0.891724\tvalid_1's auc: 0.876594\n",
      "[68]\ttraining's auc: 0.891883\tvalid_1's auc: 0.876633\n",
      "[69]\ttraining's auc: 0.892059\tvalid_1's auc: 0.876645\n",
      "[70]\ttraining's auc: 0.892187\tvalid_1's auc: 0.876673\n",
      "[71]\ttraining's auc: 0.892275\tvalid_1's auc: 0.876663\n",
      "[72]\ttraining's auc: 0.892402\tvalid_1's auc: 0.87669\n",
      "[73]\ttraining's auc: 0.892515\tvalid_1's auc: 0.876729\n",
      "[74]\ttraining's auc: 0.892637\tvalid_1's auc: 0.876729\n",
      "[75]\ttraining's auc: 0.892752\tvalid_1's auc: 0.876724\n",
      "[76]\ttraining's auc: 0.892852\tvalid_1's auc: 0.876728\n",
      "[77]\ttraining's auc: 0.892954\tvalid_1's auc: 0.876742\n",
      "[78]\ttraining's auc: 0.89306\tvalid_1's auc: 0.876744\n",
      "[79]\ttraining's auc: 0.893179\tvalid_1's auc: 0.876771\n",
      "[80]\ttraining's auc: 0.893255\tvalid_1's auc: 0.876781\n",
      "[81]\ttraining's auc: 0.893392\tvalid_1's auc: 0.876807\n",
      "[82]\ttraining's auc: 0.893462\tvalid_1's auc: 0.876843\n",
      "[83]\ttraining's auc: 0.893585\tvalid_1's auc: 0.876861\n",
      "[84]\ttraining's auc: 0.893664\tvalid_1's auc: 0.876889\n",
      "[85]\ttraining's auc: 0.893872\tvalid_1's auc: 0.87692\n",
      "[86]\ttraining's auc: 0.893975\tvalid_1's auc: 0.876936\n",
      "[87]\ttraining's auc: 0.894053\tvalid_1's auc: 0.876952\n",
      "[88]\ttraining's auc: 0.894143\tvalid_1's auc: 0.876958\n",
      "[89]\ttraining's auc: 0.894261\tvalid_1's auc: 0.876991\n",
      "[90]\ttraining's auc: 0.894435\tvalid_1's auc: 0.877006\n",
      "[91]\ttraining's auc: 0.894521\tvalid_1's auc: 0.876988\n",
      "[92]\ttraining's auc: 0.894729\tvalid_1's auc: 0.877032\n",
      "[93]\ttraining's auc: 0.894835\tvalid_1's auc: 0.877062\n",
      "[94]\ttraining's auc: 0.894951\tvalid_1's auc: 0.877066\n",
      "[95]\ttraining's auc: 0.895167\tvalid_1's auc: 0.87702\n",
      "[96]\ttraining's auc: 0.895238\tvalid_1's auc: 0.877031\n",
      "[97]\ttraining's auc: 0.895397\tvalid_1's auc: 0.877073\n",
      "[98]\ttraining's auc: 0.895511\tvalid_1's auc: 0.877114\n",
      "[99]\ttraining's auc: 0.895617\tvalid_1's auc: 0.877127\n",
      "[100]\ttraining's auc: 0.895756\tvalid_1's auc: 0.877128\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[1]\ttraining's auc: 0.867418\tvalid_1's auc: 0.868348\n",
      "[2]\ttraining's auc: 0.871894\tvalid_1's auc: 0.872479\n",
      "[3]\ttraining's auc: 0.87333\tvalid_1's auc: 0.873829\n",
      "[4]\ttraining's auc: 0.874568\tvalid_1's auc: 0.874915\n",
      "[5]\ttraining's auc: 0.875329\tvalid_1's auc: 0.875559\n",
      "[6]\ttraining's auc: 0.875869\tvalid_1's auc: 0.875869\n",
      "[7]\ttraining's auc: 0.876372\tvalid_1's auc: 0.876213\n",
      "[8]\ttraining's auc: 0.877168\tvalid_1's auc: 0.876846\n",
      "[9]\ttraining's auc: 0.877721\tvalid_1's auc: 0.877216\n",
      "[10]\ttraining's auc: 0.87815\tvalid_1's auc: 0.8775\n",
      "[11]\ttraining's auc: 0.878601\tvalid_1's auc: 0.877685\n",
      "[12]\ttraining's auc: 0.878949\tvalid_1's auc: 0.877797\n",
      "[13]\ttraining's auc: 0.879236\tvalid_1's auc: 0.87789\n",
      "[14]\ttraining's auc: 0.879747\tvalid_1's auc: 0.878094\n",
      "[15]\ttraining's auc: 0.880111\tvalid_1's auc: 0.878149\n",
      "[16]\ttraining's auc: 0.880508\tvalid_1's auc: 0.878347\n",
      "[17]\ttraining's auc: 0.880891\tvalid_1's auc: 0.878427\n",
      "[18]\ttraining's auc: 0.88117\tvalid_1's auc: 0.878508\n",
      "[19]\ttraining's auc: 0.881469\tvalid_1's auc: 0.878643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\ttraining's auc: 0.881738\tvalid_1's auc: 0.8787\n",
      "[21]\ttraining's auc: 0.882028\tvalid_1's auc: 0.878793\n",
      "[22]\ttraining's auc: 0.88233\tvalid_1's auc: 0.878833\n",
      "[23]\ttraining's auc: 0.882519\tvalid_1's auc: 0.878884\n",
      "[24]\ttraining's auc: 0.882756\tvalid_1's auc: 0.878905\n",
      "[25]\ttraining's auc: 0.882927\tvalid_1's auc: 0.878926\n",
      "[26]\ttraining's auc: 0.883162\tvalid_1's auc: 0.87895\n",
      "[27]\ttraining's auc: 0.883347\tvalid_1's auc: 0.878988\n",
      "[28]\ttraining's auc: 0.883493\tvalid_1's auc: 0.878989\n",
      "[29]\ttraining's auc: 0.883663\tvalid_1's auc: 0.879006\n",
      "[30]\ttraining's auc: 0.883889\tvalid_1's auc: 0.879008\n",
      "[31]\ttraining's auc: 0.884127\tvalid_1's auc: 0.879037\n",
      "[32]\ttraining's auc: 0.884328\tvalid_1's auc: 0.879065\n",
      "[33]\ttraining's auc: 0.884549\tvalid_1's auc: 0.879099\n",
      "[34]\ttraining's auc: 0.884731\tvalid_1's auc: 0.879102\n",
      "[35]\ttraining's auc: 0.884979\tvalid_1's auc: 0.879108\n",
      "[36]\ttraining's auc: 0.885166\tvalid_1's auc: 0.879119\n",
      "[37]\ttraining's auc: 0.88536\tvalid_1's auc: 0.879142\n",
      "[38]\ttraining's auc: 0.885551\tvalid_1's auc: 0.879133\n",
      "[39]\ttraining's auc: 0.885758\tvalid_1's auc: 0.879133\n",
      "[40]\ttraining's auc: 0.885974\tvalid_1's auc: 0.879181\n",
      "[41]\ttraining's auc: 0.886147\tvalid_1's auc: 0.879198\n",
      "[42]\ttraining's auc: 0.886429\tvalid_1's auc: 0.879255\n",
      "[43]\ttraining's auc: 0.886639\tvalid_1's auc: 0.879278\n",
      "[44]\ttraining's auc: 0.88681\tvalid_1's auc: 0.879278\n",
      "[45]\ttraining's auc: 0.886999\tvalid_1's auc: 0.879292\n",
      "[46]\ttraining's auc: 0.887265\tvalid_1's auc: 0.879368\n",
      "[47]\ttraining's auc: 0.887437\tvalid_1's auc: 0.879398\n",
      "[48]\ttraining's auc: 0.887626\tvalid_1's auc: 0.879425\n",
      "[49]\ttraining's auc: 0.887769\tvalid_1's auc: 0.879466\n",
      "[50]\ttraining's auc: 0.887962\tvalid_1's auc: 0.879504\n",
      "[51]\ttraining's auc: 0.888105\tvalid_1's auc: 0.879514\n",
      "[52]\ttraining's auc: 0.888273\tvalid_1's auc: 0.879546\n",
      "[53]\ttraining's auc: 0.888435\tvalid_1's auc: 0.879553\n",
      "[54]\ttraining's auc: 0.888678\tvalid_1's auc: 0.879584\n",
      "[55]\ttraining's auc: 0.888882\tvalid_1's auc: 0.879622\n",
      "[56]\ttraining's auc: 0.889011\tvalid_1's auc: 0.879633\n",
      "[57]\ttraining's auc: 0.889216\tvalid_1's auc: 0.879705\n",
      "[58]\ttraining's auc: 0.889394\tvalid_1's auc: 0.879719\n",
      "[59]\ttraining's auc: 0.889583\tvalid_1's auc: 0.879728\n",
      "[60]\ttraining's auc: 0.889723\tvalid_1's auc: 0.879752\n",
      "[61]\ttraining's auc: 0.889968\tvalid_1's auc: 0.879796\n",
      "[62]\ttraining's auc: 0.890159\tvalid_1's auc: 0.879834\n",
      "[63]\ttraining's auc: 0.890299\tvalid_1's auc: 0.879821\n",
      "[64]\ttraining's auc: 0.890431\tvalid_1's auc: 0.879813\n",
      "[65]\ttraining's auc: 0.890648\tvalid_1's auc: 0.879836\n",
      "[66]\ttraining's auc: 0.890863\tvalid_1's auc: 0.87988\n",
      "[67]\ttraining's auc: 0.891067\tvalid_1's auc: 0.879889\n",
      "[68]\ttraining's auc: 0.891196\tvalid_1's auc: 0.879912\n",
      "[69]\ttraining's auc: 0.891323\tvalid_1's auc: 0.879955\n",
      "[70]\ttraining's auc: 0.891433\tvalid_1's auc: 0.879966\n",
      "[71]\ttraining's auc: 0.891594\tvalid_1's auc: 0.879979\n",
      "[72]\ttraining's auc: 0.891774\tvalid_1's auc: 0.880011\n",
      "[73]\ttraining's auc: 0.891906\tvalid_1's auc: 0.880009\n",
      "[74]\ttraining's auc: 0.891989\tvalid_1's auc: 0.880024\n",
      "[75]\ttraining's auc: 0.892075\tvalid_1's auc: 0.880039\n",
      "[76]\ttraining's auc: 0.892246\tvalid_1's auc: 0.880094\n",
      "[77]\ttraining's auc: 0.892355\tvalid_1's auc: 0.880116\n",
      "[78]\ttraining's auc: 0.892437\tvalid_1's auc: 0.880169\n",
      "[79]\ttraining's auc: 0.892536\tvalid_1's auc: 0.880168\n",
      "[80]\ttraining's auc: 0.89263\tvalid_1's auc: 0.880173\n",
      "[81]\ttraining's auc: 0.892767\tvalid_1's auc: 0.880184\n",
      "[82]\ttraining's auc: 0.892923\tvalid_1's auc: 0.880266\n",
      "[83]\ttraining's auc: 0.893055\tvalid_1's auc: 0.880331\n",
      "[84]\ttraining's auc: 0.893097\tvalid_1's auc: 0.88034\n",
      "[85]\ttraining's auc: 0.893274\tvalid_1's auc: 0.880427\n",
      "[86]\ttraining's auc: 0.893422\tvalid_1's auc: 0.880435\n",
      "[87]\ttraining's auc: 0.893511\tvalid_1's auc: 0.880466\n",
      "[88]\ttraining's auc: 0.893617\tvalid_1's auc: 0.880476\n",
      "[89]\ttraining's auc: 0.893814\tvalid_1's auc: 0.880507\n",
      "[90]\ttraining's auc: 0.893917\tvalid_1's auc: 0.880502\n",
      "[91]\ttraining's auc: 0.893984\tvalid_1's auc: 0.880495\n",
      "[92]\ttraining's auc: 0.894089\tvalid_1's auc: 0.880485\n",
      "[93]\ttraining's auc: 0.894263\tvalid_1's auc: 0.880499\n",
      "[94]\ttraining's auc: 0.894339\tvalid_1's auc: 0.880503\n",
      "[95]\ttraining's auc: 0.894528\tvalid_1's auc: 0.880559\n",
      "[96]\ttraining's auc: 0.894661\tvalid_1's auc: 0.880573\n",
      "[97]\ttraining's auc: 0.894839\tvalid_1's auc: 0.880618\n",
      "[98]\ttraining's auc: 0.894876\tvalid_1's auc: 0.880624\n",
      "[99]\ttraining's auc: 0.894994\tvalid_1's auc: 0.880606\n",
      "[100]\ttraining's auc: 0.895055\tvalid_1's auc: 0.880628\n",
      "the number of probability more than 0.50 is 77570:\n",
      "the ratio of probability more than 0.50 is : 0.3878\n",
      "---------------------------------------------------\n",
      "\n",
      "the number of probability more than 0.70 is 49877:\n",
      "the ratio of probability more than 0.70 is : 0.2494\n",
      "---------------------------------------------------\n",
      "\n",
      "the number of probability more than 0.75 is 38912:\n",
      "the ratio of probability more than 0.75 is : 0.1946\n",
      "---------------------------------------------------\n",
      "\n",
      "the number of probability more than 0.80 is 28106:\n",
      "the ratio of probability more than 0.80 is : 0.1405\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Executing...\")\n",
    "Y_exam = np.zeros(X_exam.shape[0])\n",
    "k = 5\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "for tr_index, val_index in kf.split(X_processed,Y_model):\n",
    "    X_tr,Y_tr = X_processed.iloc[tr_index],Y_model.iloc[tr_index]\n",
    "    X_val, Y_val = X_processed.iloc[val_index],Y_model.iloc[val_index]\n",
    "    \n",
    "    optimized_LGBM.fit(X_tr,Y_tr,eval_metric='auc',eval_set=[(X_tr,Y_tr),(X_val,Y_val)])\n",
    "    proba = optimized_LGBM.predict_proba(X_exam_processed)[:,1]\n",
    "    Y_exam = Y_exam + proba\n",
    "Y_exam = Y_exam/k\n",
    "thresholds = np.array([0.5,0.7,0.75,0.8])\n",
    "# the ratio of high prob with different thresholds\n",
    "for num in thresholds: \n",
    "    filtered = Y_exam[np.where(Y_exam>num)]\n",
    "    print(\"the number of probability more than %.2f is %d:\" %(num,len(filtered)))\n",
    "    print(\"the ratio of probability more than %.2f is : %.4f\"%(num, float(len(filtered))/len(Y_exam)))\n",
    "    print('---------------------------------------------------\\n')\n",
    "# res = pd.DataFrame({'business prob':Y_exam})\n",
    "# res.to_csv(\"./part1.csv\")\n",
    "# the number of probability more than 0.50 is 64713:\n",
    "# the ratio of probability more than 0.50 is : 0.3236\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# the number of probability more than 0.70 is 36457:\n",
    "# the ratio of probability more than 0.70 is : 0.1823\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# the number of probability more than 0.75 is 20901:\n",
    "# the ratio of probability more than 0.75 is : 0.1045\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# the number of probability more than 0.80 is 10084:\n",
    "# the ratio of probability more than 0.80 is : 0.0504\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# the number of probability more than 0.50 is 77570:\n",
    "# the ratio of probability more than 0.50 is : 0.3878\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# the number of probability more than 0.70 is 49877:\n",
    "# the ratio of probability more than 0.70 is : 0.2494\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# the number of probability more than 0.75 is 38912:\n",
    "# the ratio of probability more than 0.75 is : 0.1946\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# the number of probability more than 0.80 is 28106:\n",
    "# the ratio of probability more than 0.80 is : 0.1405\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a61c0fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "---------------------------------------------------------\n",
      "when threshold is 0.50, the avg profit is 26583520.00\n",
      "---------------------------------------------------------\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "---------------------------------------------------------\n",
      "when threshold is 0.52, the avg profit is 26668080.00\n",
      "---------------------------------------------------------\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "when threshold is 0.53, the avg profit is 26665080.00\n",
      "---------------------------------------------------------\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04869700361211202, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04869700361211202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.24374626955501244, subsample=1.0 will be ignored. Current value: bagging_fraction=0.24374626955501244\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.411720661598768, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.411720661598768\n",
      "---------------------------------------------------------\n",
      "when threshold is 0.55, the avg profit is 26657920.00\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## task2\n",
    "for th in [0.5,0.52,0.53,0.55]:\n",
    "    profit = 0\n",
    "    for tr_index, val_index in kf.split(X_processed,Y_model):\n",
    "        X_tr,Y_tr = X_processed.iloc[tr_index],Y_model.iloc[tr_index]\n",
    "        X_val, Y_val = X_processed.iloc[val_index],Y_model.iloc[val_index]\n",
    "\n",
    "        optimized_LGBM.fit(X_tr,Y_tr,eval_metric='auc')\n",
    "        proba = optimized_LGBM.predict_proba(X_val)[:,1]\n",
    "\n",
    "        df = pd.DataFrame({'business_prob':proba},index=val_index)\n",
    "        res = pd.concat([Y_val,df],axis=1)\n",
    "        filtered = res[res['business_prob']>th]\n",
    "\n",
    "        profit += filtered['business'].sum()* 500000*0.01 - 400*filtered.shape[0]\n",
    "    # avg profit\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(\"when threshold is %.2f, the avg profit is %.2f\" %(th,profit/5))\n",
    "    print(\"---------------------------------------------------------\")\n",
    "# when threshold is 0.4, the avg profit is 25932640.00\n",
    "# when threshold is 0.45, the avg profit is 26372160.00\n",
    "# when threshold is 0.5, the avg profit is 26628040.00\n",
    "# when threshold is 0.55, the avg profit is 26650400.00\n",
    "# when threshold is 0.6, the avg profit is 26306360.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# references\n",
    "# https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "# http://devdoc.net/bigdata/LightGBM-doc-2.2.2/Parallel-Learning-Guide.html\n",
    "# https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html\n",
    "# https://www.kaggle.com/code/rmiperrier/tps-mar-lgbm-predict-proba-vs-predict/notebook\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# https://www.kaggle.com/code/kageyama/predict-by-lightgbm\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
